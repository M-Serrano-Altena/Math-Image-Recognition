{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Recognition - CROHME 2023 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert handwritten math equations in an image as latex code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 15:01:31.881350: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 15:01:31.884391: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 15:01:31.893188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 15:01:31.907418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 15:01:31.911084: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 15:01:31.922804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 15:01:32.780612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from transformers import TFViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_SEQ_LEN = 50\n",
    "AMOUNT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "missing_attrib_counter = 0\n",
    "def parse_inkml(inkml_path):\n",
    "    \"\"\"\n",
    "    Parse an .inkml file to extract the corresponding LaTeX label and image name.\n",
    "    \"\"\"\n",
    "    global missing_attrib_counter\n",
    "    tree = ET.parse(inkml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = None\n",
    "    latex_code = None\n",
    "    \n",
    "    try:\n",
    "        image_name = os.path.splitext(os.path.basename(inkml_path))[0]\n",
    "        latex_code = root.find(\".//{http://www.w3.org/2003/InkML}annotation[@type='truth']\").text\n",
    "    except AttributeError:\n",
    "        missing_attrib_counter += 1\n",
    "        print(missing_attrib_counter)\n",
    "\n",
    "    return image_name, latex_code\n",
    "\n",
    "    \n",
    "def extract_labels_from_inkml(inkml_dir):\n",
    "    \"\"\"\n",
    "    Extract labels for all .inkml files in a directory.\n",
    "    \"\"\"\n",
    "    latex_label_map = {}\n",
    "    for file in os.listdir(inkml_dir):\n",
    "        if file.endswith(\".inkml\"):\n",
    "            inkml_path = os.path.join(inkml_dir, file)\n",
    "            image_name, latex_code = parse_inkml(inkml_path)\n",
    "            if image_name is None or latex_code is None:\n",
    "                continue\n",
    "            \n",
    "            latex_label_map[image_name] = latex_code\n",
    "            \n",
    "    return latex_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_images_with_labels(img_dir, latex_label_map):\n",
    "    \"\"\"\n",
    "    Match .png images in img_dir with their LaTeX labels using the UI key.\n",
    "    \"\"\"\n",
    "    matched_data = []\n",
    "    for img_file in os.listdir(img_dir):\n",
    "        if img_file.endswith(\".png\"):\n",
    "            image_name = os.path.splitext(img_file)[0]  # Strip .png extension\n",
    "            if image_name in latex_label_map:\n",
    "                matched_data.append((os.path.join(img_dir, img_file), latex_label_map[image_name]))\n",
    "\n",
    "    return matched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str, target_size: int=IMG_SIZE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses an image by cropping the content area, padding it to make it square, \n",
    "    and resizing it to a specified target size.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        target_size (int): Target size for resizing the image. Defaults to IMG_SIZE.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Preprocessed image resized to (target_size, target_size).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Detect the bounding box of the content\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    coords = cv2.findNonZero(255 - gray_img)  # Invert the image to find non-white areas\n",
    "    x, y, w, h = cv2.boundingRect(coords)  # Get bounding box\n",
    "\n",
    "    # Crop to only keep the content area\n",
    "    cropped_img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Pad to make it square (to keep aspect ratio of handwriting the same)\n",
    "    height, width, _ = cropped_img.shape\n",
    "    max_dim = max(height, width)\n",
    "    pad_top = (max_dim - height) // 2\n",
    "    pad_bottom = max_dim - height - pad_top\n",
    "    pad_left = (max_dim - width) // 2\n",
    "    pad_right = max_dim - width - pad_left\n",
    "\n",
    "    padded_img = cv2.copyMakeBorder(\n",
    "        cropped_img,\n",
    "        pad_top, pad_bottom, pad_left, pad_right,\n",
    "        cv2.BORDER_CONSTANT,\n",
    "        value=[255, 255, 255]  # White padding\n",
    "    )\n",
    "\n",
    "    # Resize to the target size\n",
    "    resized_img = cv2.resize(padded_img, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path: str) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image, then normalizes its pixel values.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor representation of the normalized image with pixel values in [0, 1].\n",
    "    \"\"\"\n",
    "    img_arr = preprocess_image(img_path)\n",
    "    img_ds = tf.convert_to_tensor(img_arr, dtype=tf.float32)\n",
    "    img_ds /= 255.0\n",
    "    return img_ds\n",
    "\n",
    "# Function to tokenize and pad labels\n",
    "def preprocess_label(label: str, tokenizer: 'Tokenizer', max_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Tokenizes and pads a text label to a specified maximum length.\n",
    "\n",
    "    Args:\n",
    "        label (str): The input text label.\n",
    "        tokenizer (Tokenizer): A tokenizer object to encode the label.\n",
    "        max_len (int): Maximum length for the padded sequence.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A tokenized and padded representation of the label.\n",
    "    \"\"\"\n",
    "    # Tokenize the label\n",
    "    tokens = tokenizer.encode(label)\n",
    "\n",
    "    # Pad the label\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [tokens], padding='post', maxlen=max_len\n",
    "    )\n",
    "    return padded_tokens[0]  # Return the first (and only) batch\n",
    "\n",
    "# Convert your data into a TensorFlow dataset\n",
    "def create_tf_dataset(\n",
    "    matched_data: list[tuple[str, str]],\n",
    "    tokenizer: 'Tokenizer',\n",
    "    batch_size: int=BATCH_SIZE,\n",
    "    max_len: int=MAX_SEQ_LEN,\n",
    "    img_size: int=224\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from matched image paths and text labels.\n",
    "\n",
    "    Args:\n",
    "        matched_data (list of tuples): A list of (image_path, label) pairs.\n",
    "        tokenizer (Tokenizer): A tokenizer to process the text labels.\n",
    "        batch_size (int): Batch size for the dataset.\n",
    "        max_len (int): Maximum sequence length for tokenized labels.\n",
    "        img_size (int): Target size for image resizing.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A batched, shuffled, and prefetched dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_paths, labels = zip(*matched_data)\n",
    "\n",
    "    images = [load_image(image_path) for image_path in image_paths]\n",
    "    tokenized_labels = [preprocess_label(label, tokenizer, max_len) for label in labels]\n",
    "    \n",
    "    # Create the TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, tokenized_labels))\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.shuffle(buffer_size=100000)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch for better performance\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inkml_train_dir = \"CROHME23/TC11_CROHME23/INKML/train/CROHME2023_train\"\n",
    "train_labels = extract_labels_from_inkml(inkml_train_dir)\n",
    "\n",
    "img_train_dir = \"CROHME23/TC11_CROHME23/IMG/train/CROHME2013_train\" # typo in dir name: is 2023 data as well\n",
    "matched_train_data = match_images_with_labels(img_train_dir, train_labels)\n",
    "\n",
    "\n",
    "inkml_val_dir = \"CROHME23/TC11_CROHME23/INKML/val/CROHME2023_val\"\n",
    "val_labels = extract_labels_from_inkml(inkml_val_dir)\n",
    "\n",
    "img_val_dir = \"CROHME23/TC11_CROHME23/IMG/val/CROHME2023_val\"\n",
    "matched_val_data = match_images_with_labels(img_val_dir, val_labels)\n",
    "\n",
    "\n",
    "inkml_test_dir = \"CROHME23/TC11_CROHME23/INKML/test/CROHME2023_test\"\n",
    "test_labels = extract_labels_from_inkml(inkml_test_dir)\n",
    "\n",
    "img_test_dir = \"CROHME23/TC11_CROHME23/IMG/test/CROHME2023_test\"\n",
    "matched_test_data = match_images_with_labels(img_test_dir, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom Latex Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert latex code into seperate tokens that can be used by neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class LatexTokenizer:\n",
    "    def __init__(self, vocab_path: str):\n",
    "        # Load vocabulary from JSON file\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        \n",
    "        # Create token-to-ID and ID-to-token mappings\n",
    "        self.special_tokens = vocab[\"special_tokens\"]\n",
    "        self.commands = vocab[\"commands\"]\n",
    "        self.symbols = vocab[\"symbols\"]\n",
    "        self.digits = vocab[\"digits\"]\n",
    "        self.letters = vocab[\"letters\"]\n",
    "        self.environments = vocab[\"environments\"]\n",
    "        self.functions = vocab[\"functions\"]\n",
    "        self.operators = vocab[\"operators\"]\n",
    "        \n",
    "        self.tokens = (\n",
    "            self.special_tokens + self.commands + self.symbols +\n",
    "            self.digits + self.letters +\n",
    "            self.environments + self.functions +\n",
    "            self.operators\n",
    "        )\n",
    "        \n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "\n",
    "\n",
    "        self.pad_token, self.start_token, self.end_token, self.unknown_token = self.special_tokens\n",
    "\n",
    "    def _tokenize_latex(self, text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the LaTeX input text by matching tokens in the vocabulary.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            matched = False\n",
    "            # Try to find the longest match in the vocabulary\n",
    "            for token in sorted(self.tokens, key=len, reverse=True):  # Sort by length for longest match first\n",
    "                if text[i:i+len(token)] == token:\n",
    "                    tokens.append(token)\n",
    "                    i += len(token)  # Move past the matched token\n",
    "                    matched = True\n",
    "                    break  # Once a match is found, move on to the next part of the text\n",
    "            if not matched:\n",
    "                # If no match, treat as unknown token or skip\n",
    "                tokens.append(self.unknown_token)\n",
    "                i += 1  # Move past the unmatched character\n",
    "        return tokens\n",
    "        \n",
    "    def text_to_ids(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Tokenizes a LaTeX string into a sequence of IDs.\n",
    "        \"\"\"\n",
    "        text = re.sub(\"\\s*\", \"\", text)\n",
    "        tokens = self._tokenize_latex(text)\n",
    "        ids = [self.token_to_id.get(token, self.token_to_id[self.unknown_token]) for token in tokens]\n",
    "        return [self.token_to_id[self.start_token]] + ids + [self.token_to_id[self.end_token]]\n",
    "    \n",
    "    def ids_to_text(self, ids: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Converts a sequence of IDs back into a LaTeX string.\n",
    "        \"\"\"\n",
    "        tokens = [self.id_to_token.get(i, self.unknown_token) for i in ids]\n",
    "        return \" \".join(tokens).replace(self.start_token, \"\").replace(self.end_token, \"\").strip()\n",
    "\n",
    "    # aliases\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return self.text_to_ids(text)\n",
    "\n",
    "    def decode(self, ids: list[int]) -> list[str]:\n",
    "        return self.ids_to_text(ids)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734271297.019775   96478 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 15:01:37.022239: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-12-15 15:02:54.927162: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1384857600 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "path_to_latex_vocab = \"latex_vocab.json\"\n",
    "tokenizer = LatexTokenizer(path_to_latex_vocab)\n",
    "\n",
    "START_TOKEN = tokenizer.start_token\n",
    "END_TOKEN = tokenizer.end_token\n",
    "\n",
    "\n",
    "train_dataset = create_tf_dataset(matched_train_data, tokenizer, batch_size=BATCH_SIZE, max_len=MAX_SEQ_LEN)\n",
    "val_dataset = create_tf_dataset(matched_val_data, tokenizer, batch_size=BATCH_SIZE, max_len=MAX_SEQ_LEN)\n",
    "test_dataset = create_tf_dataset(matched_test_data, tokenizer, batch_size=BATCH_SIZE, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 4, 151, 285, 152, 151, 286, 152, 142, 9, 30, 147, 191, 31, 148, 2]\n",
      "Decoded LaTeX: \\frac { x } { y } + \\sin \\left ( \\theta \\right )\n",
      "Vocabulary Size: 366\n"
     ]
    }
   ],
   "source": [
    "# Example LaTeX string\n",
    "latex_input = \"\\\\frac{x}{y} + \\\\sin\\\\left(\\\\theta\\\\right)\"\n",
    "\n",
    "# Tokenize and detokenize\n",
    "token_ids = tokenizer.text_to_ids(latex_input)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "decoded_latex = tokenizer.ids_to_text(token_ids)\n",
    "print(\"Decoded LaTeX:\", decoded_latex)\n",
    "\n",
    "# Vocabulary size\n",
    "print(\"Vocabulary Size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTModel.\n",
      "\n",
      "All the weights of TFViTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained Vision Transformer\n",
    "vit = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "patch_size = 16\n",
    "seq_len = (IMG_SIZE // patch_size)**2 + 1\n",
    "hidden_size = 768\n",
    "output_shape_model = (seq_len, hidden_size)\n",
    "\n",
    "def preprocess(inputs: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Preprocesses input images for the Vision Transformer model by scaling pixel values \n",
    "    to [0, 1] and normalizing them in \"torch\" mode.\n",
    "\n",
    "    Args:\n",
    "        inputs (tf.Tensor): Input images as tensors.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Preprocessed tensor images ready for the Vision Transformer.\n",
    "    \"\"\"\n",
    "    # Scale pixel values to the range [0, 1] and normalize using \"torch\" mode\n",
    "    inputs = tf.keras.applications.imagenet_utils.preprocess_input(inputs, mode=\"torch\")\n",
    "    # Convert KerasTensor to TensorFlow Tensor explicitly\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    return inputs\n",
    "\n",
    "def get_vit_encoder() -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a Vision Transformer (ViT) encoder model using pre-trained weights.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A ViT encoder model with input shape (IMG_SIZE, IMG_SIZE, AMOUNT_CHANNELS).\n",
    "    \"\"\"\n",
    "    input_layer = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, AMOUNT_CHANNELS), name=\"image_input\")\n",
    "\n",
    "    # Preprocess inputs using the updated function\n",
    "    # transpose because for some reason model flips the order, so tranposing here to compensate\n",
    "    processed_inputs = tf.keras.layers.Lambda(lambda x: tf.transpose(x, perm=[0, 3, 1, 2]))(input_layer)\n",
    "    \n",
    "    # Ensure inputs are compatible with TFViTModel\n",
    "    outputs = tf.keras.layers.Lambda(lambda x: vit(x).last_hidden_state, output_shape=output_shape_model)(processed_inputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=outputs, name=\"vit_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(vocab_size: int, hidden_size: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Creates a GRU-based decoder for sequence generation.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Vocabulary size for the output sequences.\n",
    "        hidden_size (int): Hidden size for the GRU and embedding layers.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A decoder model for generating sequences.\n",
    "    \"\"\"\n",
    "    decoder_inputs = layers.Input(shape=(None,))\n",
    "    encoder_outputs = layers.Input(shape=(None, hidden_size))  # ViT feature size\n",
    "\n",
    "    embedding = layers.Embedding(input_dim=vocab_size, output_dim=hidden_size)(decoder_inputs)\n",
    "    gru_output = layers.GRU(hidden_size, return_sequences=True)(embedding, initial_state=None)\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(gru_output)\n",
    "\n",
    "    return tf.keras.Model(inputs=[decoder_inputs, encoder_outputs], outputs=outputs, name=\"decoder\")\n",
    "\n",
    "def get_decoder_with_soft_tokens(vocab_size: int, hidden_size: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Creates a GRU-based decoder that accepts soft token inputs (probability distributions).\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Vocabulary size for the output sequences.\n",
    "        hidden_size (int): Hidden size for the GRU and embedding layers.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A decoder model for generating sequences from soft tokens.\n",
    "    \"\"\"\n",
    "    decoder_inputs = layers.Input(shape=(None, vocab_size))  # Soft tokens as input (probabilities)\n",
    "    encoder_outputs = layers.Input(shape=(None, hidden_size))  # ViT feature size\n",
    "\n",
    "    # Create the embedding layer\n",
    "    embedding_matrix = layers.Embedding(input_dim=vocab_size, output_dim=hidden_size)\n",
    "\n",
    "    # Use a Lambda layer for the weighted sum calculation\n",
    "    def weighted_sum(inputs):\n",
    "        # `inputs` contains the soft tokens (decoder_inputs) and the embedding weights\n",
    "        soft_tokens, embeddings = inputs\n",
    "        return tf.matmul(soft_tokens, embeddings)\n",
    "\n",
    "    # Pass the embedding weights dynamically to the Lambda layer\n",
    "    embedded_inputs = layers.Lambda(weighted_sum, output_shape=(None, hidden_size))(\n",
    "        [decoder_inputs, embedding_matrix(tf.range(vocab_size))]\n",
    "    )\n",
    "\n",
    "    # GRU and Dense layers\n",
    "    gru_output = layers.GRU(hidden_size, return_sequences=True)(embedded_inputs, initial_state=None)\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(gru_output)\n",
    "\n",
    "    return tf.keras.Model(inputs=[decoder_inputs, encoder_outputs], outputs=outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class ScheduledSamplingLayer(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Scheduled sampling layer that blends ground truth tokens and predicted tokens for training.\n",
    "        Args:\n",
    "            temperature: Initial temperature for Gumbel-Softmax.\n",
    "        \"\"\"\n",
    "        super(ScheduledSamplingLayer, self).__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def gumbel_softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Perform Gumbel-Softmax sampling.\n",
    "        Args:\n",
    "            logits: Tensor of shape (batch_size, vocab_size), un-normalized logits.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, vocab_size), softmax probabilities with Gumbel noise.\n",
    "        \"\"\"\n",
    "        uniform_noise = tf.random.uniform(tf.shape(logits), minval=1e-6, maxval=1.0)\n",
    "        gumbel_noise = -tf.math.log(-tf.math.log(uniform_noise))\n",
    "        noisy_logits = logits + gumbel_noise\n",
    "        return tf.nn.softmax(noisy_logits / self.temperature)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Scheduled sampling during training.\n",
    "        Args:\n",
    "            inputs: A dictionary containing:\n",
    "                - \"token_inputs\": Ground truth token sequence, shape (batch_size, sequence_length).\n",
    "                - \"decoder_output_logits\": Decoder's predicted logits, shape (batch_size, vocab_size).\n",
    "                - \"time_step\": Current time step as a scalar.\n",
    "                - \"teacher_forcing_prob\": Probability of using teacher forcing.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, vocab_size), logits or probabilities for the next step.\n",
    "        \"\"\"\n",
    "        token_inputs = inputs[\"token_inputs\"]\n",
    "        decoder_output = inputs[\"decoder_output\"]\n",
    "        time_step = inputs[\"time_step\"]\n",
    "        teacher_forcing_prob = inputs[\"teacher_forcing_prob\"]\n",
    "        \n",
    "        teacher_forcing_prob_scalar = tf.reduce_mean(teacher_forcing_prob)\n",
    "\n",
    "        # Extract the ground truth token for the current time step\n",
    "        token_input_t = tf.cast(tf.gather(token_inputs, time_step, axis=1), dtype=tf.int32)\n",
    "\n",
    "        # One-hot encode the ground truth token for blending\n",
    "        token_input_t_one_hot = tf.one_hot(token_input_t, depth=tf.shape(decoder_output)[-1])\n",
    "        token_input_t_one_hot = tf.expand_dims(token_input_t_one_hot, axis=1)\n",
    "\n",
    "        # Blend ground truth and predicted probabilities\n",
    "        blended_logits = teacher_forcing_prob_scalar * tf.math.log(token_input_t_one_hot + 1e-8) + \\\n",
    "                         (1.0 - teacher_forcing_prob_scalar) * decoder_output\n",
    "\n",
    "        # Apply Gumbel-Softmax for differentiable sampling\n",
    "        sampled_probabilities = self.gumbel_softmax(blended_logits)\n",
    "\n",
    "        # Return probabilities for training, tokens for inference\n",
    "        next_token_probability = sampled_probabilities[:, time_step, :]\n",
    "        next_token_probability = tf.expand_dims(next_token_probability, axis=1)\n",
    "        \n",
    "        return next_token_probability\n",
    "\n",
    "class ConcatenateLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer to concatenate decoder inputs with the next time step input.\n",
    "    \"\"\"\n",
    "    def call(self, inputs: tuple[tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Concatenates the two input tensors along axis 1.\n",
    "        \"\"\"\n",
    "        decoder_inputs, next_input_t = inputs\n",
    "        return tf.concat([decoder_inputs, next_input_t], axis=1)\n",
    "\n",
    "class ConcatenateLayerSingle(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer to concatenate a list of tensors along a specified axis.\n",
    "    \"\"\"\n",
    "    def call(self, input_list: list[tf.Tensor], axis: int=1) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Concatenates a list of input tensors along the specified axis.\n",
    "        \"\"\"\n",
    "        return tf.concat(input_list, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a model consisting of a Vision Transformer (ViT) encoder, a GRU-based decoder, \n",
    "    and scheduled sampling for sequence generation.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the output vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Keras model.\n",
    "    \"\"\"\n",
    "    encoder = get_vit_encoder()\n",
    "    decoder = get_decoder_with_soft_tokens(vocab_size, hidden_size=512)\n",
    "\n",
    "    image_inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, AMOUNT_CHANNELS), name=\"image_inputs\")\n",
    "    token_inputs = layers.Input(shape=(None,), name=\"token_inputs\")  # Teacher forcing tokens\n",
    "    teacher_forcing_prob = layers.Input(shape=(), name=\"teacher_forcing_prob\", dtype=tf.float32)  # Teacher forcing probability\n",
    "    \n",
    "    encoder_outputs = encoder(image_inputs)\n",
    "    encoder_outputs = layers.Dense(512)(encoder_outputs)\n",
    "    \n",
    "    start_token_id = tokenizer.token_to_id[START_TOKEN]\n",
    "    decoder_inputs_prob_distrib = tf.one_hot(tf.fill([BATCH_SIZE, 1], start_token_id), vocab_size)\n",
    "\n",
    "    scheduled_sampling = ScheduledSamplingLayer()\n",
    "    concatenate_layer = ConcatenateLayer()\n",
    "    concatenate_layer_single = ConcatenateLayerSingle()\n",
    "\n",
    "    accumulated_tokens = decoder_inputs_prob_distrib\n",
    "\n",
    "    for t in range(MAX_SEQ_LEN - 1):\n",
    "        t_tensor = tf.convert_to_tensor(t, dtype=tf.int32)  # Ensure `t` is a tensor\n",
    "        \n",
    "        # Pass inputs to the decoder\n",
    "        decoder_output_t = decoder([accumulated_tokens, encoder_outputs])\n",
    "        \n",
    "        # Apply scheduled sampling\n",
    "        next_token_prob_distrib = scheduled_sampling({\n",
    "            \"token_inputs\": token_inputs, \n",
    "            \"decoder_output\": decoder_output_t, \n",
    "            \"time_step\": t_tensor,  # Pass as a tensor\n",
    "            \"teacher_forcing_prob\": teacher_forcing_prob\n",
    "        })\n",
    "\n",
    "        # accumulate all previous tokens\n",
    "        accumulated_tokens = concatenate_layer([accumulated_tokens, next_token_prob_distrib])\n",
    "\n",
    "    decoder_output_t = decoder([accumulated_tokens, encoder_outputs])\n",
    "    return tf.keras.Model(\n",
    "        inputs=[image_inputs, token_inputs, teacher_forcing_prob],\n",
    "        outputs=accumulated_tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=tokenizer.vocab_size)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient exists for variable kernel, shape: (512, 1536)\n",
      "Gradient exists for variable recurrent_kernel, shape: (512, 1536)\n",
      "Gradient exists for variable bias, shape: (2, 1536)\n",
      "Gradient exists for variable kernel, shape: (512, 366)\n",
      "Gradient exists for variable bias, shape: (366,)\n"
     ]
    }
   ],
   "source": [
    "# Mock inputs\n",
    "hidden_layer = 512\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "decoder_inputs_mock = tf.random.uniform([BATCH_SIZE, MAX_SEQ_LEN, vocab_size], dtype=tf.float32)\n",
    "encoder_outputs_mock = tf.random.uniform([BATCH_SIZE, MAX_SEQ_LEN, hidden_layer], dtype=tf.float32)\n",
    "\n",
    "# Run the model on mock inputs\n",
    "with tf.GradientTape() as tape:\n",
    "    decoder = get_decoder_with_soft_tokens(vocab_size, hidden_layer)\n",
    "    tape.watch(decoder_inputs_mock)  # Ensure gradients are being tracked\n",
    "    tape.watch(encoder_outputs_mock)\n",
    "    decoder_output_mock = decoder([decoder_inputs_mock, encoder_outputs_mock])\n",
    "\n",
    "# Check if gradients exist for Dense layer kernel and bias\n",
    "grads = tape.gradient(decoder_output_mock, decoder.trainable_variables)\n",
    "for grad, var in zip(grads, decoder.trainable_variables):\n",
    "    if grad is None:\n",
    "        print(f\"WARNING: No gradient for variable {var.name}\")\n",
    "    else:\n",
    "        print(f\"Gradient exists for variable {var.name}, shape: {grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataset to return both inputs as required by model\n",
    "def modify_dataset_for_model(dataset, teacher_forcing_prob):\n",
    "    return dataset.map(lambda img, label: ((tf.cast(img, dtype=tf.float32), tf.cast(label, dtype=tf.int32), tf.cast(tf.fill(tf.shape(label)[:1], teacher_forcing_prob), dtype=tf.float32)), tf.cast(label, dtype=tf.int32)))\n",
    "\n",
    "# the fraction of tokens that are trained on is ground truth,\n",
    "# other part is predicted tokens with the model\n",
    "teacher_forcing_prob = 0.5\n",
    "\n",
    "train_dataset_model = modify_dataset_for_model(train_dataset, teacher_forcing_prob)\n",
    "val_dataset_model = modify_dataset_for_model(val_dataset, teacher_forcing_prob)\n",
    "test_dataset_model = modify_dataset_for_model(test_dataset, teacher_forcing_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcsa/.local/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['image_inputs', 'token_inputs', 'teacher_forcing_prob']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n",
      "/home/marcsa/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:731: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/32\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 7s/step - accuracy: 0.6574 - loss: 3.2159  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m test_dataset_model\n\u001b[1;32m      8\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m      9\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m         patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     11\u001b[0m         restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "models_path = \"../models/CROHME\"\n",
    "\n",
    "if train_model:\n",
    "    del test_dataset\n",
    "    del test_dataset_model\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset_model,\n",
    "        validation_data=val_dataset_model,\n",
    "        epochs=250,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    history = None\n",
    "    latest_version_num = 6\n",
    "    latest_version = [file_name for file_name in sorted(os.listdir(models_path), reverse=True) if f\"00{latest_version_num}\" in file_name][0]\n",
    "    latest_version_path = os.path.join(models_path, latest_version)\n",
    "    print(latest_version_path)\n",
    "    model.load_weights(latest_version_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "\n",
    "def is_existing_file(path, counter):\n",
    "    return os.path.isfile(os.path.join(path, f\"char-recog-model_crohme_{counter:03d}.weights.h5\"))\n",
    "\n",
    "while is_existing_file(models_path, counter):\n",
    "    counter += 1\n",
    "\n",
    "if train_model:\n",
    "    model.save_weights(os.path.join(models_path, f\"char-recog-model_crohme_{counter:03d}.weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "if history is not None:\n",
    "    history_frame = pd.DataFrame(history.history)\n",
    "    history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "    history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if history is not None:\n",
    "loss, accuracy = model.evaluate(test_dataset_model)\n",
    "print(loss, accuracy)\n",
    "print(test_dataset_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_n_images(X, y=None, n=10, row_spacing=0.5):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Show the first n images if available, otherwise show all images\n",
    "    for i in range(min(n, X.shape[0])):  \n",
    "        plt.subplot(n // 5 + 1, 5, i+1)\n",
    "        plt.imshow(X[i], cmap='gray')  # Assuming 28x28 images\n",
    "        if y is not None:\n",
    "            plt.title(label_map[y[i]])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(hspace=row_spacing)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_label_tuple, label in train_dataset_model:\n",
    "    predictions = model.predict(image_label_tuple)\n",
    "    print(predictions.shape)\n",
    "    predicted_tokens = np.argmax(predictions, axis=-1)\n",
    "    for token_seq in predicted_tokens:\n",
    "        print(tokenizer.decode([token for token in token_seq if token != 0]))\n",
    "\n",
    "    plot_n_images(image_label_tuple[0].numpy().astype(np.float32))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoregressive(\n",
    "    model: tf.keras.Model, \n",
    "    dataset: tf.data.Dataset, \n",
    "    start_token: str, \n",
    "    end_token: str, \n",
    "    max_length: int = MAX_SEQ_LEN\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model using an autoregressive decoding strategy.\n",
    "    (generating a token based on all previous tokens)\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained sequence generation model.\n",
    "        dataset (tf.data.Dataset): A dataset of image inputs and true tokens.\n",
    "        start_token (str): The start token for decoding.\n",
    "        end_token (str): The end token for decoding.\n",
    "        max_length (int): The maximum sequence length to decode. Defaults to MAX_SEQ_LEN.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Encode the start and end tokens\n",
    "    start_token_encoded = tokenizer.encode(start_token)[0]\n",
    "    end_token_encoded = tokenizer.encode(end_token)[0]\n",
    "    \n",
    "    for (image_inputs, true_tokens) in dataset:\n",
    "        batch_size = image_inputs.shape[0]\n",
    "        \n",
    "        # Initialize generated tokens as integers\n",
    "        generated_tokens = tf.fill((batch_size, 1), start_token_encoded)  # Start token as an ID\n",
    "\n",
    "        # Pass image inputs through the encoder\n",
    "        encoder_outputs = model.get_layer(\"vit_encoder\")(image_inputs)\n",
    "        \n",
    "        # Apply the extra Dense layer to match the decoder input shape\n",
    "        encoder_outputs = model.get_layer(\"dense_1\")(encoder_outputs)  # Apply Dense layer\n",
    "        \n",
    "        # Autoregressive decoding loop\n",
    "        for _ in range(max_length):\n",
    "            # Predict the next token\n",
    "            decoder_outputs = model.get_layer(\"decoder\")([generated_tokens, encoder_outputs])\n",
    "            next_tokens = tf.argmax(decoder_outputs[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "            print(next_tokens)\n",
    "            \n",
    "            generated_tokens = tf.concat([generated_tokens, tf.expand_dims(next_tokens, axis=-1)], axis=-1)\n",
    "\n",
    "            # Stop if all sequences have reached the end token\n",
    "            if tf.reduce_all(next_tokens == end_token_encoded):\n",
    "                break\n",
    "\n",
    "        # Compare generated tokens with true tokens (excluding padding and special tokens)\n",
    "        for gen, true in zip(generated_tokens.numpy(), true_tokens.numpy()):\n",
    "            # Strip padding and end tokens for comparison\n",
    "            gen = gen[:np.where(gen == end_token_encoded)[0][0]] if end_token_encoded in gen else gen\n",
    "            true = true[:np.where(true == end_token_encoded)[0][0]] if end_token_encoded in true else true\n",
    "            if np.array_equal(gen, true):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Autoregressive accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# evaluate_autoregressive(model, test_dataset, START_TOKEN, END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_test = load_image(\"CROHME23/TC11_CROHME23/IMG/test/CROHME2023_test/form_5_175_E874.png\")\n",
    "# image_test = np.squeeze(image_test, axis=0)\n",
    "plt.imshow(image_test, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
