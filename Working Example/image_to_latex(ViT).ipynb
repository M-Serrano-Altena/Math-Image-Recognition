{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source code: https://drive.google.com/file/d/1ZU1u3NnPduGHmIwiu8nSBoj5gYRPSjEq/view?source=post_page-----13fc4ce253d7--------------------------------\n",
    "\n",
    "Original Article: https://dohyeongkim.medium.com/image-to-latex-using-vision-transformer-13fc4ce253d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "U8l4RJ0XRPEm",
    "outputId": "fdb72d43-7967-4a8c-d79d-c89bdb8bce8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /home/marcsa/.local/lib/python3.12/site-packages (0.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:40:43.847500: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-05 22:40:43.850740: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-05 22:40:43.857015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733434843.868738    3608 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733434843.872273    3608 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 22:40:43.886385: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "!pip install einops\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5e_SigQFiWf"
   },
   "source": [
    "### Choose a dataset\n",
    "\n",
    "This tutorial is set up to give a choice of datasets. Either [Flickr8k](https://www.ijcai.org/Proceedings/15/Papers/593.pdf) or a small slice of the [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) dataset. These two are downloaded and converted from scratch, but it wouldn't be hard to convert the tutorial to use the caption datasets available in [TensorFlow Datasets](https://www.tensorflow.org/datasets): [Coco Captions](https://www.tensorflow.org/datasets/catalog/coco_captions) and the full [Conceptual Captions](https://www.tensorflow.org/datasets/community_catalog/huggingface/conceptual_captions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-1FUqsZlzmJ5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "\n",
    "data_root = 'latex_data'\n",
    "vocab = open(os.path.join(data_root, \"latex_vocab.txt\")).readlines()\n",
    "formulae = open(os.path.join(data_root, \"formulas.norm.lst\"), 'r').readlines()\n",
    "\n",
    "char_to_idx = {x.split('\\n')[0]:i for i, x in enumerate(vocab)}\n",
    "char_to_idx['#UNK'] = len(char_to_idx)\n",
    "char_to_idx['#START'] = len(char_to_idx)\n",
    "char_to_idx['#END'] = len(char_to_idx)\n",
    "idx_to_char = {y:x for x, y in char_to_idx.items()}\n",
    "\n",
    "file_name = 'train.lst'\n",
    "data_path = os.path.join(data_root, file_name)\n",
    "file_list = open(data_path, 'r')\n",
    "\n",
    "image_dir = os.path.join(data_root, 'images_processed')\n",
    "\n",
    "set_list = []\n",
    "missing = {}\n",
    "for i, line in enumerate(file_list):\n",
    "    form = formulae[int(line.split()[1])].strip().split()\n",
    "\n",
    "    out_form = [char_to_idx['#START']]\n",
    "    for c in form:\n",
    "        try:\n",
    "            out_form += [char_to_idx[c]]\n",
    "        except:\n",
    "            if c not in missing:\n",
    "                print(c, \" not found!\")\n",
    "                missing[c] = 1\n",
    "            else:\n",
    "                missing[c] += 1\n",
    "\n",
    "            out_form += [char_to_idx['#UNK']]\n",
    "\n",
    "    out_form += [char_to_idx['#END']]\n",
    "    set_list.append([line.split()[0], out_form])\n",
    "\n",
    "    image_file_name = line.split()[0]\n",
    "    label = out_form\n",
    "\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7b0xK5pczmJ5"
   },
   "outputs": [],
   "source": [
    "class LatexDataset:\n",
    "    def __init__(self, set='train', batch_size=32):\n",
    "        self.data_root = 'latex_data'\n",
    "\n",
    "        self.set = 'train'\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_dict = np.load(os.path.join(data_root, set + '_buckets.npy'), allow_pickle=True).tolist()\n",
    "\n",
    "        self.data_length = np.sum([len(self.train_dict[x]) for x in self.train_dict.keys()])\n",
    "        print(\"Length of %s data: \" % set, self.data_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for keys in self.train_dict.keys():\n",
    "            train_list = self.train_dict[keys]\n",
    "            N_FILES = (len(train_list) // self.batch_size) * self.batch_size\n",
    "            for batch_idx in range(0, N_FILES, self.batch_size):\n",
    "                train_sublist = train_list[batch_idx:batch_idx + self.batch_size]\n",
    "                imgs = []\n",
    "                input_tokens = []\n",
    "                label_tokens = []\n",
    "                for x, y in train_sublist:\n",
    "                    img = Image.open(os.path.join(self.data_root, 'images_processed/') + x)\n",
    "                    img = img.resize((image_size, image_size))\n",
    "\n",
    "                    img = np.asarray(img)[:,:,0][:,:,None] / 255.0\n",
    "\n",
    "                    imgs.append(img)\n",
    "                    input_tokens.append(y[:-1])\n",
    "                    label_tokens.append(y[1:])\n",
    "\n",
    "                imgs = np.asarray(imgs, dtype=np.float32).transpose(0, 1, 2, 3)\n",
    "                lens = [len(x) for x in input_tokens]\n",
    "\n",
    "                Y_input_tokens = np.zeros((self.batch_size, max(lens)), dtype=np.int32)\n",
    "                for i, input_token in enumerate(input_tokens):\n",
    "                    Y_input_tokens[i, :len(input_token)] = input_token\n",
    "\n",
    "                Y_label_tokens = np.zeros((self.batch_size, max(lens)), dtype=np.int32)\n",
    "                for i, label_token in enumerate(label_tokens):\n",
    "                    Y_label_tokens[i, :len(label_token)] = label_token\n",
    "\n",
    "                yield imgs, Y_input_tokens, Y_label_tokens\n",
    "\n",
    "    __call__ = __iter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DJnqUU5wzmJ5",
    "outputId": "9cba25bf-be1d-48bf-b8ae-7ad5755a429a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data:  76511\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3608/3005810904.py:2: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "Length of test data:  10355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733434845.548611    3608 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_ds_gen = LatexDataset(set='train', batch_size=32)\n",
    "train_ds = tf.data.Dataset.from_generator(train_ds_gen, (tf.float32, tf.int32, tf.int32))\n",
    "\n",
    "test_ds_gen = LatexDataset(set='test', batch_size=32)\n",
    "test_ds = tf.data.Dataset.from_generator(test_ds_gen, (tf.float32, tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tIGy0i12zmJ5",
    "outputId": "59da71e9-4489-4e0f-fec5-5efbfddf9035",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install cv2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m displayPreds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m Y: display(Math(Y\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#END\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     13\u001b[0m properties \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "from IPython.display import Image as ipythonImage\n",
    "from io import StringIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "!pip install cv2\n",
    "import cv2\n",
    "\n",
    "displayPreds = lambda Y: display(Math(Y.split('#END')[0]))\n",
    "\n",
    "properties = np.load(os.path.join(data_root, 'properties.npy'), allow_pickle=True).tolist()\n",
    "\n",
    "vocab = open(os.path.join(data_root, \"latex_vocab.txt\")).readlines()\n",
    "\n",
    "word_to_index = {x.split('\\n')[0]:i for i, x in enumerate(vocab)}\n",
    "word_to_index['#UNK'] = len(word_to_index)\n",
    "word_to_index['#START'] = len(word_to_index)\n",
    "word_to_index['#END'] = len(word_to_index)\n",
    "index_to_word = {y:x for x, y in word_to_index.items()}\n",
    "\n",
    "index_to_words = lambda Y: ' '.join(map(lambda x: properties['idx_to_char'][x], Y))\n",
    "\n",
    "n = int(32)\n",
    "plt.figure(figsize=(40, 40))\n",
    "\n",
    "x = None\n",
    "for idx, train_data in enumerate(train_ds):\n",
    "    imgs, Y_input_tokens, Y_label_tokens = train_data\n",
    "\n",
    "    print(\"imgs.shape: \", imgs.shape)\n",
    "    print(\"Y_input_tokens.shape: \", Y_input_tokens.shape)\n",
    "    print(\"Y_label_tokens.shape: \", Y_label_tokens.shape)\n",
    "\n",
    "    sub_idx = -1\n",
    "\n",
    "    img = imgs[sub_idx]\n",
    "    Y_input_token = Y_input_tokens[sub_idx]\n",
    "    Y_label_token = Y_label_tokens[sub_idx]\n",
    "    print(\"Y_input_token: \", Y_input_token)\n",
    "    print(\"Y_label_token: \", Y_label_token)\n",
    "\n",
    "    #ax = plt.subplot(n, n, sub_idx + 1)\n",
    "    #patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "    plt.imshow(img.numpy(), cmap=\"gray\")\n",
    "    #plt.imshow(img.numpy())\n",
    "\n",
    "    #print(\"img.numpy().shape: \", img.numpy().shape)\n",
    "\n",
    "    #cv2.imshow(img.numpy())\n",
    "    #plt.axis(\"off\")\n",
    "    #print(\"img.numpy().shape: \", img.numpy().shape)\n",
    "\n",
    "    #preds_chars = index_to_words(Y[1:].numpy()).replace('$','')\n",
    "    #preds_chars = preds_chars.split('#END')[0]\n",
    "    #print(\"preds_chars: \", preds_chars)\n",
    "    #print(\"\")\n",
    "    if idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WmkC2fWzmJ6",
    "outputId": "ad3573c0-2a04-4c4d-b395-370beecd603f"
   },
   "outputs": [],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "### Image feature extractor\n",
    "\n",
    "You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlUckK8Zfikv",
    "outputId": "6872a3b9-5dc7-4133-97b0-d8fc65a0df11",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE_SHAPE = (image_size, image_size * 4, 1)\n",
    "mobilenet = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=IMAGE_SHAPE),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], padding='same', activation='relu', use_bias=False),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=[2, 2]),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], padding='same', activation='relu', use_bias=False),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=[2, 2]),\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], padding='same', activation='relu', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], padding='same', activation='relu', use_bias=False),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[1, 2], strides=[1, 2]),\n",
    "        tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], padding='same', activation='relu', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2, 1], strides=[2, 1]),\n",
    "        tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], padding='same', activation='relu', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    ]\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dojkiou9gL3R"
   },
   "source": [
    "Here's a function to load an image and resize it for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXR0217aRPFR"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JyQ7zS6gzZh"
   },
   "source": [
    "The model returns a feature map for each image in the input batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTOKKm8izmJ7",
    "outputId": "12133fb7-eefd-4916-8358-a838f920ab88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root = 'latex_data'\n",
    "set = 'test'\n",
    "test_dict = np.load(os.path.join(data_root, set + '_buckets.npy'), allow_pickle=True).tolist()\n",
    "test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1q84EGV9zmJ7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex_path = test_dict[(400, 160)][0][0]\n",
    "image_dir = os.path.join(data_root, 'images_processed')\n",
    "image_dir = os.path.join(image_dir, ex_path)\n",
    "\n",
    "img = Image.open(image_dir).convert('YCbCr')\n",
    "img = img.resize((image_size * 4, image_size))\n",
    "img = np.asarray(img)[:,:,0][:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sY86n2i6wJNm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
    "#print(mobilenet(np.expand_dims(img, 0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQ9bO_FPzmJ7",
    "outputId": "f5ea6eee-1491-4763-e38f-d12177b1e5a3"
   },
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "patch_size = 6\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 512\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim * 2, projection_dim]\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = tf.keras.layers.Dense(units, activation=tf.keras.activations.gelu)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images=images, sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = img\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "print(\"image.shape: \", image.shape)\n",
    "resized_image = tf.image.resize(tf.convert_to_tensor([image]), size=(image_size, image_size))\n",
    "print(\"resized_image.shape: \", resized_image.shape)\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "print(\"n: \", n)\n",
    "'''\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_a1gtJdzmJ8"
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = tf.keras.Input(shape=(image_size, image_size, 1))\n",
    "\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        x2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    #representation = tf.keras.layers.Flatten()(representation)\n",
    "    #representation = tf.keras.layers.Dropout(0.5)(representation)\n",
    "    #features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=representation)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WplU-HsYzmJ8"
   },
   "outputs": [],
   "source": [
    "vit_classifier = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhSt4gXlzmJ8",
    "outputId": "f562c792-1a93-410c-d33b-c1eeb2927506"
   },
   "outputs": [],
   "source": [
    "ex_path = test_dict[(400, 160)][0][0]\n",
    "image_dir = os.path.join(data_root, 'images_processed')\n",
    "image_dir = os.path.join(image_dir, ex_path)\n",
    "\n",
    "img = Image.open(image_dir).convert('YCbCr')\n",
    "img = img.resize((image_size, image_size))\n",
    "img = np.asarray(img)[:,:,0][:,:,None]\n",
    "\n",
    "print(vit_classifier(np.expand_dims(img, 0)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "### Prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT-kXv-6zmJ8",
    "outputId": "a7f71bc6-e8fe-45c6-c4dc-e006abd3e9e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "step = 0\n",
    "for train_batch in train_ds:\n",
    "    print(\"train_batch[0][0].shape: \", train_batch[0][0].shape)\n",
    "    print(\"train_batch[1][0]: \", train_batch[1][0])\n",
    "    img = train_batch[0][0]\n",
    "    result = mobilenet(np.expand_dims(img, 0))\n",
    "    print(result.shape)\n",
    "\n",
    "    step += 1\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_nnebCjzmJ8",
    "outputId": "af6ad8b1-647f-4fe0-c8b7-fb94394326b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "test_data = []\n",
    "for test_batch in test_ds.random(seed=4).take(5):\n",
    "\n",
    "    print(\"test_batch: \", test_batch)\n",
    "    #print(\"test_batch[1][0]: \", test_batch[1][0])\n",
    "\n",
    "    #img = test_batch[0][0]\n",
    "    #result = mobilenet(np.expand_dims(img, 0))\n",
    "    #print(result.shape)\n",
    "\n",
    "    step += 1\n",
    "    break\n",
    "\n",
    "print(\"step: \", step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfICM49WFpIb"
   },
   "source": [
    "## A Transformer decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P91LU2F0a9Ga"
   },
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "    self.token_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=depth, mask_zero=True)\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JTLiX3lKooQ"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x, use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "\n",
    "    return self.layernorm(x)\n",
    "\n",
    "\n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x, y, **kwargs):\n",
    "    # x.shape:  TensorShape([1, 29, 256])\n",
    "    # y.shape:  TensorShape([1, 1600, 512])\n",
    "    #tf.print(\"x.shape: \", x.shape)\n",
    "    #tf.print(\"y.shape: \", y.shape)\n",
    "\n",
    "    attn, attention_scores = self.mha(query=x, value=y, return_attention_scores=True)\n",
    "    self.last_attention_scores = attention_scores\n",
    "    x = self.add([x, attn])\n",
    "\n",
    "    return self.layernorm(x)\n",
    "\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=units, dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,key_dim=units, dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "\n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40v3lc3dzmJ9"
   },
   "outputs": [],
   "source": [
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocabulary_size, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=vocabulary_size, **kwargs)\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5j1rw3nzmJ9"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(word_to_index)\n",
    "output_layer = TokenOutput(vocabulary_size, banned_tokens=('', '#UNK', '#START'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHCISYehH1f6"
   },
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  def __init__(self, vocabulary_size, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=200, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "\n",
    "    vocab = open(os.path.join(data_root, \"latex_vocab.txt\")).readlines()\n",
    "    self.word_to_index = {x.split('\\n')[0]:i for i, x in enumerate(vocab)}\n",
    "    self.word_to_index['#UNK'] = len(self.word_to_index)\n",
    "    self.word_to_index['#START'] = len(self.word_to_index)\n",
    "    self.word_to_index['#END'] = len(self.word_to_index)\n",
    "    self.index_to_word = {y:x for x, y in self.word_to_index.items()}\n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(vocab_size=vocabulary_size, depth=units, max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate) for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer\n",
    "\n",
    "  def call(self, image, txt):\n",
    "    #image.shape 1:  TensorShape([1, 160, 640, 1])\n",
    "\n",
    "    image = self.feature_extractor(image)\n",
    "    #image.shape 2:  TensorShape([1, 20, 80, 512])\n",
    "\n",
    "    #image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "    #image.shape 3:  TensorShape([1, 1600, 512])\n",
    "\n",
    "    txt = self.seq_embedding(txt)\n",
    "\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "\n",
    "    txt = self.output_layer(txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "  def simple_gen(self, image, temperature=1):\n",
    "    initial = self.word_to_index['#START'] # (batch, sequence)\n",
    "    initial = tf.expand_dims(initial, 0)\n",
    "    initial = tf.expand_dims(initial, 0)\n",
    "\n",
    "    tokens = initial # (batch, sequence)\n",
    "\n",
    "    for n in range(50):\n",
    "      preds = self(image[tf.newaxis, ...], tokens).numpy()  # (batch, sequence, vocab)\n",
    "      preds = preds[:,-1, :]  #(batch, vocab)\n",
    "      if temperature == 0:\n",
    "          next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "      else:\n",
    "          next = tf.random.categorical(preds / temperature, num_samples=1)  # (batch, 1)\n",
    "\n",
    "      next = tf.cast(next, tf.int32)\n",
    "\n",
    "      tokens = tf.concat([tokens, next], axis=1) # (batch, sequence)\n",
    "\n",
    "      if next[0] == self.word_to_index['#END']:\n",
    "        break\n",
    "\n",
    "    words = []\n",
    "    for token in tokens[0, 1:-1]:\n",
    "        word = index_to_word[token.numpy()]\n",
    "        words.append(word)\n",
    "\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "\n",
    "    return result.numpy().decode()\n",
    "\n",
    "model = Captioner(vocabulary_size, feature_extractor=create_vit_classifier(), output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=4, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4RyzB4RzmJ-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex_path = test_dict[(400, 160)][0][0]\n",
    "image_dir = os.path.join(data_root, 'images_processed')\n",
    "image_dir = os.path.join(image_dir, ex_path)\n",
    "\n",
    "image = Image.open(image_dir).convert('YCbCr')\n",
    "image = image.resize((image_size * 4, image_size))\n",
    "image = np.asarray(image)[:,:,0][:,:,None]\n",
    "\n",
    "#for t in (0.0, 0.5, 1.0):\n",
    "#  result = model.simple_gen(image, temperature=t)\n",
    "#  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DCsJ5D3zmJ-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for ds in train_ds:\n",
    "#    logits = model(ds[0], ds[1])\n",
    "#    print(\"logits.shape: \", logits.shape)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0FpTvaPkqON"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcwZdqObK-U"
   },
   "source": [
    "To train the model you'll need several additional components:\n",
    "\n",
    "- The Loss and metrics\n",
    "- The Optimizer\n",
    "- Optional Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5IW2mWa2sAG"
   },
   "source": [
    "### Losses and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbpbDQTw1lOW"
   },
   "source": [
    "Here's an implementation of a masked loss and accuracy:\n",
    "\n",
    "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s24im3FqxAfT"
   },
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):\n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8)\n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss * mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels != 0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOhjHqgv3F2e"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dyQN9UfJYEd"
   },
   "source": [
    "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKDwbZOCZ-AP"
   },
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    ex_path = test_dict[(400, 160)][0][0]\n",
    "    image_dir = os.path.join(data_root, 'images_processed')\n",
    "    image_dir = os.path.join(image_dir, ex_path)\n",
    "\n",
    "    self.image = Image.open(image_dir).convert('YCbCr')\n",
    "    self.image = self.image.resize((image_size, image_size))\n",
    "    self.image = np.asarray(self.image)[:,:,0][:,:,None]\n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    for t in (0.0, 0.5, 1.0):\n",
    "      result = self.model.simple_gen(self.image, temperature=t)\n",
    "      print(result)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yNA3_RAsdl0"
   },
   "source": [
    "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGVLpzo13rcA",
    "outputId": "d7aa731a-2e69-42ac-c1a3-367967cd0505"
   },
   "outputs": [],
   "source": [
    "g = GenerateText()\n",
    "g.model = model\n",
    "g.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAxp4KZRKDk9"
   },
   "source": [
    "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjzrwGZp23xx"
   },
   "outputs": [],
   "source": [
    "callbacks = [GenerateText(), tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JErsJ2zhzmKD"
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(1e-4, int(100000 / 32.0 * 1000), 1e-6)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "@tf.function\n",
    "def train(ds):\n",
    "    labels = ds[2]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(ds[0], ds[1])\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n",
    "\n",
    "        mask = (labels != 0) & (loss < 1e8)\n",
    "        mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "        loss = loss * mask\n",
    "        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    #(grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test(ds):\n",
    "    labels = ds[2]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(ds[0], ds[1])\n",
    "\n",
    "        mask = tf.cast(labels != 0, tf.float32)\n",
    "        preds = tf.argmax(preds, axis=-1)\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        match = tf.cast(preds == labels, mask.dtype)\n",
    "        acc = tf.reduce_sum(match * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBaJhQpcG8u0"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM1QkyPbzmKD",
    "outputId": "0ed05cb2-f0f0-4d3f-ebcf-0e25b03570b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"tensorboard\")\n",
    "\n",
    "for epoch in range(0, 1000000):\n",
    "    print(\"epoch: \", epoch)\n",
    "\n",
    "    train_losses = []\n",
    "    step = 0\n",
    "    for ds_train in train_ds:\n",
    "        loss = train(ds_train)\n",
    "        train_losses.append(loss)\n",
    "        step += 1\n",
    "\n",
    "    model.save_weights('model/model_' + str(epoch))\n",
    "\n",
    "    mean_loss_train = np.mean(train_losses)\n",
    "    mean_perp_train = np.mean(list(map(lambda x: np.power(np.e, x), train_losses)))\n",
    "\n",
    "    test_accuracies = []\n",
    "    for ds_test in test_ds:\n",
    "        accuracy = test(ds_test)\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "    mean_accuracy_test = np.mean(test_accuracies)\n",
    "\n",
    "    print(\"Mean train loss:\", mean_loss_train, \", Mean train perplexity:\", mean_perp_train, \"Mean test Accuracy:\", mean_accuracy_test)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"mean_loss_train\", mean_loss_train, step=epoch)\n",
    "        tf.summary.scalar(\"mean_perp_train\", mean_perp_train, step=epoch)\n",
    "        tf.summary.scalar(\"mean_accuracy_test\", mean_accuracy_test, step=epoch)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALRhPaIxzmKD"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhNsQiA7zmKD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model.load_weights('model/model_464')\n",
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "\n",
    "data_root = 'latex_data'\n",
    "set = 'test'\n",
    "test_dict = np.load(os.path.join(data_root, set + '_buckets.npy'), allow_pickle=True).tolist()\n",
    "data_length = np.sum([len(test_dict[x]) for x in test_dict.keys()])\n",
    "print(\"Length of %s data: \" % set, data_length)\n",
    "\n",
    "key_list = list(test_dict.keys())\n",
    "key = random.choice(key_list)\n",
    "test_list = test_dict[key]\n",
    "test_image_info = random.choice(test_list)\n",
    "\n",
    "img = Image.open(os.path.join(data_root, 'images_processed/') + test_image_info[0])\n",
    "img = img.resize((image_size * 4, image_size))\n",
    "img = np.asarray(img)[:,:,0][:,:,None] / 255.0\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "Y = np.array(test_image_info[1])\n",
    "\n",
    "preds_chars = index_to_words(Y[1:]).replace('$','')\n",
    "preds_chars = preds_chars.split('#END')[0]\n",
    "\n",
    "print(\"Label: \")\n",
    "displayPreds(preds_chars)\n",
    "\n",
    "result = model.simple_gen(img, temperature=0.0)\n",
    "print(\"Prediction: \")\n",
    "displayPreds(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQN1qT7KNqbL"
   },
   "source": [
    "## Attention plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9XJaC2b2J23"
   },
   "source": [
    "Now, using the trained model,  run that `simple_gen` method on the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fi61Y2JzmKD"
   },
   "outputs": [],
   "source": [
    "ex_path = test_dict[(400, 160)][0][0]\n",
    "image_dir = os.path.join(data_root, 'images_processed')\n",
    "image_dir = os.path.join(image_dir, ex_path)\n",
    "\n",
    "image = Image.open(image_dir).convert('YCbCr')\n",
    "image = image.resize((256, 256))\n",
    "image = np.asarray(img)[:,:,0][:,:,None] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UQPtNTb2eu3"
   },
   "outputs": [],
   "source": [
    "result = model.simple_gen(image, temperature=0.0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvUTJXXQzmKE"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "displayPreds = lambda Y: display(Math(Y))\n",
    "\n",
    "step = 0\n",
    "\n",
    "fig = plt.figure(figsize=(100, 100))\n",
    "\n",
    "for ds in test_ds.shuffle(1000):\n",
    "    images = ds[0]\n",
    "\n",
    "    num_image = images.shape[0]\n",
    "    for i in range(8):\n",
    "      result = model.simple_gen(images[i].numpy(), temperature=0.0)\n",
    "      displayPreds(result)\n",
    "\n",
    "      grid_size = max(int(np.ceil(num_image / 9)), 9)\n",
    "      ax = fig.add_subplot(6, grid_size, i + 1)\n",
    "      img = ax.imshow(images[i].numpy(), cmap='gray')\n",
    "\n",
    "    step += 1\n",
    "    break\n",
    "    #if step == 20:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NXbmeLGN1bJ"
   },
   "source": [
    "Split the output back into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHKOpm0w5Xto"
   },
   "outputs": [],
   "source": [
    "str_tokens = result.split()\n",
    "str_tokens.append('[END]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE-AjuAV55Qo"
   },
   "source": [
    "The `DecoderLayers` each cache the attention scores for their `CrossAttention` layer. The shape of each attention map is `(batch=1, heads, sequence, image)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZpyuQvq2q-B"
   },
   "outputs": [],
   "source": [
    "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
    "[map.shape for map in attn_maps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T42ImsWv6oHG"
   },
   "source": [
    "So stack the maps along the `batch` axis, then average over the `(batch, heads)` axes, while splitting the `image` axis back into `height, width`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojwtvnkh6mS-"
   },
   "outputs": [],
   "source": [
    "attention_maps = tf.concat(attn_maps, axis=0)\n",
    "attention_maps = einops.reduce(\n",
    "    attention_maps,\n",
    "    'batch heads sequence (height width) -> sequence height width',\n",
    "    height=7, width=7,\n",
    "    reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TM7rA3zGpJW"
   },
   "source": [
    "Now you have a single attention map, for each sequence prediction. The values in each map should sum to `1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASWmWerGCZp3"
   },
   "outputs": [],
   "source": [
    "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv7XYGFUd-U7"
   },
   "source": [
    "So here is where the model was focusing attention while generating each token of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fD_y7PD6RPGt"
   },
   "outputs": [],
   "source": [
    "def plot_attention_maps(image, str_tokens, attention_map):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "    len_result = len(str_tokens)\n",
    "\n",
    "    titles = []\n",
    "    for i in range(len_result):\n",
    "      map = attention_map[i]\n",
    "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "      ax = fig.add_subplot(3, grid_size, i+1)\n",
    "      titles.append(ax.set_title(str_tokens[i]))\n",
    "      img = ax.imshow(image)\n",
    "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
    "                clim=[0.0, np.max(map)])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PI4NAAws9rvY"
   },
   "outputs": [],
   "source": [
    "plot_attention_maps(image / 255, str_tokens, attention_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riTz0abQKMkV"
   },
   "source": [
    "Now put that together into a more usable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mktpfW-SKQIJ"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def run_and_show_attention(self, image, temperature=0.0):\n",
    "  result_txt = self.simple_gen(image, temperature)\n",
    "  str_tokens = result_txt.split()\n",
    "  str_tokens.append('[END]')\n",
    "\n",
    "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
    "  attention_maps = tf.concat(attention_maps, axis=0)\n",
    "  attention_maps = einops.reduce(\n",
    "      attention_maps,\n",
    "      'batch heads sequence (height width) -> sequence height width',\n",
    "      height=7, width=7,\n",
    "      reduction='mean')\n",
    "\n",
    "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
    "  t = plt.suptitle(result_txt)\n",
    "  t.set_y(1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FntRkY11OiMw"
   },
   "outputs": [],
   "source": [
    "run_and_show_attention(model, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rprk3HEvZuxb"
   },
   "source": [
    "## Try it on your own images\n",
    "\n",
    "For fun, below you're provided a method you can use to caption your own images with the model you've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for strange results!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Psd1quzaAWg"
   },
   "outputs": [],
   "source": [
    "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
    "image_path = tf.keras.utils.get_file(origin=image_url)\n",
    "image = load_image(image_path)\n",
    "\n",
    "run_and_show_attention(model, image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
