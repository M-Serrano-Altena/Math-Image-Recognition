# Assessment

## Bijzondere Code
Vooral de code bij de double-english en CROHME zijn het meest bijzonder. De double-english is het best werkende model en CROHME had veel potentie om een goed model te zijn. Verder is de processing bij EMNIST wel interessant omdat de EMNIST data een bijzondere verwerking heeft. Daarom moesten de eigen afbeeldingen ook op dezelfde manier verwerkt worden. 

## Belangrijke Beslissingen
De eerste belangrijke beslissing was om de EMNIST data niet te gebruiken bij het voorspellen van losstaande letters en cijfers. De EMNIST data is namelijk een van de meest bekende en uitgebreide datasets om cijfers en letters te voorspellen. Het probleem waar ik tegenaan liep met deze dataset is dat de data op een bijzondere manier verwerkt is. Als eerst zijn de afbeeldingen $28 \times 28$ in plaats van $128 \times 128$ zoals de andere datasets. Dit betekent dat als de EMNIST data toegevoegd zou worden, dat de andere afbeeldingen resolutie zouden moeten verliezen. Maar nog belangrijker, de EMNIST data is alle kanten op geroteerd en gespiegeld. Dit zorgde juist voor verwarring bij het model met het herkennen. Bijvoorbeeld een **M** werd herkent als een **W** omdat het model getraind had op afbeeldingen van een omgekeerde **M**. De voorspellingen zijn beter geworden door de EMNIST data weg te halen, dus lijkt dit mij een goede beslissing geweest te zijn.

Daarna was een andere grote beslissing om niet een wiskunde formule te splitten in enkele componenten voor herkenning. Oftewel, ik heb niet verder gewerkt met het *Double English* model die losstaande characters kon voorspellen. Het leek mij beter om meteen één groot model te maken die de afbeeldingen in een keer omzet naar latex (dit is dus de CROHME notebook). Vooral omdat bij het splitten van characters het mij moeilijk leek om de attention toe te voegen die het onderliggend verband tussen de symbolen in de gaten houdt. Hiermee bedoel ik bijvoorbeeld dat bij de formule $x^2$ rekening moet houden dat de $2$ een macht is van $x$. Het is lastig om te zeggen of dit de juiste keuze is gebleken omdat de ViT uiteindelijk niet binnen de tijd zinvolle resultaten heeft kunnen geven.

De laatste grote beslissing was om een eigen latex tokenizer te maken. Eerst maakte ik gebruik van een bestaande tokenizer voor algemene taal. Maar uiteindelijk gebruikte dit te veel geheugen en had te veel mogelijkheden voor ons doel. Het hield ook niet verschillende componenten bij elkaar die bij elkaar hoorden te blijven (bijvoorbeeld \frac werd gesplitst in \ en frac). Weer is het lastig om te zeggen of dit echt de juiste keuze is gebleken, maar hoogst waarschijnlijk heeft dit wel geholpen. 